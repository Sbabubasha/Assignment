import requests
from bs4 import BeautifulSoup
import pandas as pd

def scrapefrom(url):
    # Fetch HTML content
    response = requests.get(url)
    html = response.text

    # Parse HTML
    soup = BeautifulSoup(html, 'html.parser')

    # Extract data from the table
    records = []
    table = soup.find('table')
    for row in table.find_all('tr')[1:]:  # Skip the header row
        cols = row.find_all('td')
        if len(cols) >= 8:  # Assuming each record has at least 8 columns
            record = {
                'S.No': cols[0].text.strip(),
                'Promoter Name': cols[1].text.strip(),
                'Project Name': cols[2].text.strip(),
                'RERA Reg.No.': cols[3].text.strip(),
                'ProjectType': cols[4].text.strip(),
                'District':cols[5].text.strip(),
                'StartDate': cols[6].text.strip(),
                'EndDate': cols[7].text.strip() ,
                'Details' : cols[8].find('a')['href'] if cols[8].find('a') else None
            }
            records.append(record)

    # Load existing Excel data (if any)
    try:
        existing_data = pd.read_excel('news.xlsx')
    except FileNotFoundError:
        existing_data = pd.DataFrame()

    # Convert new records to a DataFrame
    new_data = pd.DataFrame(records)

    # Check for duplicates and append new data
    filtered_data = new_data[~new_data.duplicated()]
    combined_data = pd.concat([existing_data, filtered_data], ignore_index=True)

    # Save the combined data to Excel
    combined_data.to_excel('news.xlsx', index=False)

    print('Data has been scraped and stored in news.xlsx.')

if __name__ == "__main__":
    url = 'https://www.up-rera.in/projects'
    scrapefrom(url)

def detailsfun(url):
    x=[]
    page=requests.get(url)
    soup=BeautifulSoup(page.content,'html.parser')
 
    quote_elements=soup.find('div',class_="col-lg-12 col-md-12 col-sm-12")
    x+=quote_elements.text.split('\n')
    s=[]
    
    for i in range(len(x)):
        if x[i]!='':
            s.append(x[i])
    project_dict = {}
    for i in range(0, len(s), 2):
        key = s[i]
        value = s[i + 1]
        project_dict[key] = value
    # print(project_dict)
    df = pd.DataFrame(project_dict.items(), columns=['Key', 'Value'])

    # Save the DataFrame to an Excel file
    excel_filename = 'results_data.xlsx'
    df.to_excel(excel_filename, index=False)
    print(f"Data has been converted and saved to {excel_filename}")


def scrapefrom(url):
    # Fetch HTML content
    response = requests.get(url)
    html = response.text
    # Parse HTML
    soup = BeautifulSoup(html, 'html.parser')
    # Extract data from the table
    records = []
    table = soup.find('table')
    for row in table.find_all('tr')[1:]:  # Skip the header row
        cols = row.find_all('td')
        record=[]
        if len(cols) >= 1:  # Assuming each record has at least 8 columns
            record.append(cols[8].find('a')['href'] if cols[8].find('a') else None)
        records.append(record)
    n=[]
    for i in records:
        for j in i:
            n.append(j)
            detailsfun('https://www.up-rera.in/'+j)
    print('over')
    # print(record['Details'])
    # print(records)
    print('Data has been scraped and stored in news.xlsx.')

if __name__ == "__main__":
    url = 'https://www.up-rera.in/projects'
    scrapefrom(url)
